<!DOCTYPE html>
<html lang="zh-CN">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Transformer网络解读 | 叶子的技术碎碎念</title>
    <meta name="generator" content="VuePress 1.9.10">
    
    <meta name="description" content="叶子的技术碎碎念">
    
    <link rel="preload" href="/assets/css/0.styles.795fcb47.css" as="style"><link rel="preload" href="/assets/js/app.bd8004a5.js" as="script"><link rel="preload" href="/assets/js/7.d7dc3e09.js" as="script"><link rel="preload" href="/assets/js/2.d701f7d6.js" as="script"><link rel="preload" href="/assets/js/1.f54fd289.js" as="script"><link rel="preload" href="/assets/js/46.7cf49e15.js" as="script"><link rel="prefetch" href="/assets/js/10.79681363.js"><link rel="prefetch" href="/assets/js/11.29920769.js"><link rel="prefetch" href="/assets/js/14.b2cffd01.js"><link rel="prefetch" href="/assets/js/15.ba187924.js"><link rel="prefetch" href="/assets/js/16.1f560dd7.js"><link rel="prefetch" href="/assets/js/17.199dc857.js"><link rel="prefetch" href="/assets/js/18.b7c7ff02.js"><link rel="prefetch" href="/assets/js/19.04465c2e.js"><link rel="prefetch" href="/assets/js/20.4ba280e8.js"><link rel="prefetch" href="/assets/js/21.19e0a90d.js"><link rel="prefetch" href="/assets/js/22.79ef54ac.js"><link rel="prefetch" href="/assets/js/23.ba4ab7a0.js"><link rel="prefetch" href="/assets/js/24.afd714df.js"><link rel="prefetch" href="/assets/js/25.86016902.js"><link rel="prefetch" href="/assets/js/26.0a94d433.js"><link rel="prefetch" href="/assets/js/27.2768e2e4.js"><link rel="prefetch" href="/assets/js/28.c9fabea7.js"><link rel="prefetch" href="/assets/js/29.1e376621.js"><link rel="prefetch" href="/assets/js/3.bb090a50.js"><link rel="prefetch" href="/assets/js/30.129ca9e0.js"><link rel="prefetch" href="/assets/js/31.e32e2d81.js"><link rel="prefetch" href="/assets/js/32.79e0bedf.js"><link rel="prefetch" href="/assets/js/33.a9925ae6.js"><link rel="prefetch" href="/assets/js/34.b93dc917.js"><link rel="prefetch" href="/assets/js/35.2852e413.js"><link rel="prefetch" href="/assets/js/36.fb410a04.js"><link rel="prefetch" href="/assets/js/37.6bbe274a.js"><link rel="prefetch" href="/assets/js/38.1a1b315a.js"><link rel="prefetch" href="/assets/js/39.0e2e0de5.js"><link rel="prefetch" href="/assets/js/4.91310cca.js"><link rel="prefetch" href="/assets/js/40.7d41e87d.js"><link rel="prefetch" href="/assets/js/41.6d506318.js"><link rel="prefetch" href="/assets/js/42.384ae83a.js"><link rel="prefetch" href="/assets/js/43.848ec979.js"><link rel="prefetch" href="/assets/js/44.6ad8702d.js"><link rel="prefetch" href="/assets/js/45.d9165949.js"><link rel="prefetch" href="/assets/js/47.ab137e3c.js"><link rel="prefetch" href="/assets/js/48.23e89325.js"><link rel="prefetch" href="/assets/js/49.2e5727e6.js"><link rel="prefetch" href="/assets/js/5.ae9af707.js"><link rel="prefetch" href="/assets/js/50.e98bddca.js"><link rel="prefetch" href="/assets/js/51.a27effc2.js"><link rel="prefetch" href="/assets/js/52.ce7c3e96.js"><link rel="prefetch" href="/assets/js/53.959c70d1.js"><link rel="prefetch" href="/assets/js/6.ddc4768c.js"><link rel="prefetch" href="/assets/js/8.de91a9be.js"><link rel="prefetch" href="/assets/js/9.f7cbd4f5.js"><link rel="prefetch" href="/assets/js/vendors~docsearch.65214fb5.js">
    <link rel="stylesheet" href="/assets/css/0.styles.795fcb47.css">
  </head>
  <body>
    <div id="app" data-server-rendered="true"><div class="theme-container" data-v-7dd95ae2><div data-v-7dd95ae2><div class="password-shadow password-wrapper-out" style="display:none;" data-v-59e6cb88 data-v-7dd95ae2 data-v-7dd95ae2><h3 class="title" data-v-59e6cb88>叶子的技术碎碎念</h3> <p class="description" data-v-59e6cb88>叶子的技术碎碎念</p> <label id="box" class="inputBox" data-v-59e6cb88><input type="password" value="" data-v-59e6cb88> <span data-v-59e6cb88>Konck! Knock!</span> <button data-v-59e6cb88>OK</button></label> <div class="footer" data-v-59e6cb88><span data-v-59e6cb88><i class="iconfont reco-theme" data-v-59e6cb88></i> <a target="blank" href="https://vuepress-theme-reco.recoluan.com" data-v-59e6cb88>vuePress-theme-reco</a></span> <span data-v-59e6cb88><i class="iconfont reco-copyright" data-v-59e6cb88></i> <a data-v-59e6cb88><!---->
          
        <!---->
        2023
      </a></span></div></div> <div class="hide" data-v-7dd95ae2><header class="navbar" data-v-7dd95ae2><div class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/" class="home-link router-link-active"><!----> <span class="site-name">叶子的技术碎碎念</span></a> <div class="links"><div class="color-picker"><a class="color-button"><i class="iconfont reco-color"></i></a> <div class="color-picker-menu" style="display:none;"><div class="mode-options"><h4 class="title">Choose mode</h4> <ul class="color-mode-options"><li class="dark">dark</li><li class="auto active">auto</li><li class="light">light</li></ul></div></div></div> <div class="search-box"><i class="iconfont reco-search"></i> <input aria-label="Search" autocomplete="off" spellcheck="false" value=""> <!----></div> <nav class="nav-links can-hide"><div class="nav-item"><a href="/" class="nav-link"><i class="undefined"></i>
  关于我
</a></div><div class="nav-item"><a href="/AI/courses/DeepLearning Specialization/01 Neural Networks and Deep Learning/深度学习与神经网络入门.html" class="nav-link"><i class="undefined"></i>
  人工智能
</a></div><div class="nav-item"><a href="/Java/Map.html" class="nav-link"><i class="undefined"></i>
  Java
</a></div><div class="nav-item"><div class="dropdown-wrapper"><a class="dropdown-title"><span class="title"><i class="undefined"></i>
      叶子的技术碎碎念
    </span> <span class="arrow right"></span></a> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="https://github.com/careywyr" target="_blank" rel="noopener noreferrer" class="nav-link external"><i class="undefined"></i>
  Github
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li><li class="dropdown-item"><!----> <a href="https://juejin.cn/user/2831954919569245" target="_blank" rel="noopener noreferrer" class="nav-link external"><i class="undefined"></i>
  掘金
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li></ul></div></div> <!----></nav></div></header> <div class="sidebar-mask" data-v-7dd95ae2></div> <aside class="sidebar" data-v-7dd95ae2><div class="personal-info-wrapper" data-v-1fad0c41 data-v-7dd95ae2><!----> <!----> <div class="num" data-v-1fad0c41><div data-v-1fad0c41><h3 data-v-1fad0c41>15</h3> <h6 data-v-1fad0c41>文章</h6></div> <div data-v-1fad0c41><h3 data-v-1fad0c41>0</h3> <h6 data-v-1fad0c41>标签</h6></div></div> <ul class="social-links" data-v-1fad0c41></ul> <hr data-v-1fad0c41></div> <nav class="nav-links"><div class="nav-item"><a href="/" class="nav-link"><i class="undefined"></i>
  关于我
</a></div><div class="nav-item"><a href="/AI/courses/DeepLearning Specialization/01 Neural Networks and Deep Learning/深度学习与神经网络入门.html" class="nav-link"><i class="undefined"></i>
  人工智能
</a></div><div class="nav-item"><a href="/Java/Map.html" class="nav-link"><i class="undefined"></i>
  Java
</a></div><div class="nav-item"><div class="dropdown-wrapper"><a class="dropdown-title"><span class="title"><i class="undefined"></i>
      叶子的技术碎碎念
    </span> <span class="arrow right"></span></a> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="https://github.com/careywyr" target="_blank" rel="noopener noreferrer" class="nav-link external"><i class="undefined"></i>
  Github
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li><li class="dropdown-item"><!----> <a href="https://juejin.cn/user/2831954919569245" target="_blank" rel="noopener noreferrer" class="nav-link external"><i class="undefined"></i>
  掘金
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li></ul></div></div> <!----></nav> <ul class="sidebar-links"><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading open"><span>Coursera深度学习专项课程</span> <span class="arrow down"></span></p> <ul class="sidebar-links sidebar-group-items"><li><a href="/AI/courses/DeepLearning Specialization/01 Neural Networks and Deep Learning/深度学习与神经网络入门.html" class="sidebar-link">深度学习与神经网络入门</a></li><li><a href="/AI/courses/DeepLearning Specialization/02 Improving Deep Neural Networks- Hyperparameter Tuning, Regularization and Optimization/Week1-blog.html" class="sidebar-link">深度学习实践基础</a></li><li><a href="/AI/courses/DeepLearning Specialization/02 Improving Deep Neural Networks- Hyperparameter Tuning, Regularization and Optimization/Week2-blog.html" class="sidebar-link">深度学习的优化算法</a></li><li><a href="/AI/courses/DeepLearning Specialization/02 Improving Deep Neural Networks- Hyperparameter Tuning, Regularization and Optimization/Week3-blog.html" class="sidebar-link">超参数调优、批量归一化以及深度学习框架</a></li><li><a href="/AI/courses/DeepLearning Specialization/03 Structuring Machine Learning Projects/BLOG.html" class="sidebar-link">机器学习策略详解</a></li><li><a href="/AI/courses/DeepLearning Specialization/05 Sequence Models/Week1 blog.html" class="sidebar-link">循环神经网络介绍</a></li><li><a href="/AI/courses/DeepLearning Specialization/05 Sequence Models/Week2 blog.html" class="sidebar-link">浅谈 Word Embeddings</a></li><li><a href="/AI/courses/DeepLearning Specialization/05 Sequence Models/Week3 blog.html" class="sidebar-link">解密序列到序列模型：从机器翻译到语音识别</a></li><li><a href="/AI/courses/DeepLearning Specialization/05 Sequence Models/Week4 blog.html" class="active sidebar-link">Transformer 网络解读</a></li></ul></section></li><li><section class="sidebar-group depth-0"><p class="sidebar-heading"><span>机器学习基础</span> <!----></p> <ul class="sidebar-links sidebar-group-items"><li><a href="/AI/机器学习/关联规则挖掘与Apriori算法.html" class="sidebar-link">关联规则挖掘与Apriori算法</a></li></ul></section></li></ul> </aside> <div class="password-shadow password-wrapper-in" style="display:none;" data-v-59e6cb88 data-v-7dd95ae2><h3 class="title" data-v-59e6cb88>Transformer网络解读</h3> <!----> <label id="box" class="inputBox" data-v-59e6cb88><input type="password" value="" data-v-59e6cb88> <span data-v-59e6cb88>Konck! Knock!</span> <button data-v-59e6cb88>OK</button></label> <div class="footer" data-v-59e6cb88><span data-v-59e6cb88><i class="iconfont reco-theme" data-v-59e6cb88></i> <a target="blank" href="https://vuepress-theme-reco.recoluan.com" data-v-59e6cb88>vuePress-theme-reco</a></span> <span data-v-59e6cb88><i class="iconfont reco-copyright" data-v-59e6cb88></i> <a data-v-59e6cb88><!---->
          
        <!---->
        2023
      </a></span></div></div> <div data-v-7dd95ae2><div data-v-7dd95ae2><main class="page" style="padding-right:0;"><section style="display:;"><div class="page-title"><h1 class="title">Transformer网络解读</h1> <div data-v-8a445198><i class="iconfont reco-account" data-v-8a445198><span data-v-8a445198>叶子</span></i> <i class="iconfont reco-date" data-v-8a445198><span data-v-8a445198>2023/8/26</span></i> <!----> <!----></div></div> <div class="theme-reco-content content__default"><p>终于到序列模型课程最后一周的内容了，本篇博客依然是基于Andrew Ng的深度学习专项课程的序列模型来编写的，本篇内容不会很多，主要就是Transformer网络相关的知识点，Transformer网络是一种基于注意力机制的神经网络架构,被广泛应用于自然语言处理领域,尤其是机器翻译任务中。本文将详细介绍Transformer网络的关键概念和工作原理。废话不多说，现在开始吧。</p> <h2 id="transformer-网络介绍"><a href="#transformer-网络介绍" class="header-anchor">#</a> Transformer 网络介绍</h2> <p>我们前面讲解过在序列模型中常用的技术包括RNN、GRU和LSTM，这些模型虽然解决了一些问题，包括梯度消失、长距离依赖等，但模型的复杂度也随之增加了不少。它们都是顺序模型，会将输入一个词或一个标记地顺序处理。显然这样的处理性能是比较弱的。</p> <p>Transformer架构创新性地将注意力机制和CNN相结合，允许我们对整个序列进行并行计算，可以一次处理整个句子，而不是从左到右逐词处理。它的核心理念主要是<strong>自注意力（Self Attention）和多头注意力(Multi-Head Attention)</strong> 这两点。
简单来说，如果我们有一个包含5个词的句子，自注意的目标是并行地为这五个词计算出五个标识。而多头注意力就是对自注意力过程进行循环，从而得到这些表示的多个版本，这些表示是非常丰富的，可以用于机器翻译或其他NLP任务。</p> <h2 id="self-attention"><a href="#self-attention" class="header-anchor">#</a> Self-Attention</h2> <p>自注意力(Self-Attention)机制是Transformer网络的核心组件。它可以对序列进行并行计算,为序列中的每个词汇生成一个注意力向量,表示其在特定上下文中的含义。</p> <p>自注意力机制可以帮助我们理解每个单词在特定上下文中的含义。比如，&quot;Africa&quot;这个词，在不同的上下文中可能代表历史兴趣的地点，或者假期的目的地，或者世界第二大洲。自注意力机制会根据周围的单词来确定在此句子中我们谈论的&quot;非洲&quot;的最合适的表示方式。</p> <p><img src="https://leafw-blog-pic.oss-cn-hangzhou.aliyuncs.com/1693034113221.png" alt=""></p> <p>自注意力机制为序列中每个词汇计算出一个<strong>Query向量、Key向量和Value向量</strong>。其具体计算步骤如下:</p> <ol><li>首先，我们将每个单词与查询、键和值对应起来。这些对由学习到的矩阵和该单词的嵌入向量相乘得到。</li> <li>查询可以看作是对单词的一个问题，例如，对&quot;Africa&quot;的问题可能是&quot;那里发生了什么？&quot;。</li> <li>我们计算查询和每个键的内积，来确定其他单词对查询问题的回答的质量。</li> <li>我们对所有内积结果进行softmax运算，这样可以获得每个单词的注意力值。</li> <li>最后，我们将得到的softmax值与相应单词的值向量相乘，并将结果相加，得到的就是我们需要的自注意力表示。</li></ol> <p>自注意力机制的优点在于可以根据整个序列的上下文来获得每个词汇的表示,而非仅依赖于临近词汇。同时,其并行计算性质也大大提升了模型的训练和推理效率。</p> <p>我们对序列中的所有单词执行上述计算过程，以获取相应的自注意力表示。最后，所有这些计算可以由Attention(Q, K, V)进行概括，其中Q，K，V是所有查询、键和值的矩阵。值得注意的是,这里Query、Key、Value矩阵的计算都使用了不同的权重矩阵,这使得自注意力机制可以学习输入序列的不同表示。</p> <p><img src="https://leafw-blog-pic.oss-cn-hangzhou.aliyuncs.com/1693034342218.png" alt=""></p> <p>自注意力机制的结果是每个词的表示都更为丰富和细致，因为它考虑了每个词左右的上下文。</p> <h2 id="multi-head-attention"><a href="#multi-head-attention" class="header-anchor">#</a> Multi-Head Attention</h2> <p>Multi-Head Attention 机制对自注意力机制进行拓展，允许模型联合学习序列的不同表示子空间。</p> <p>多头注意力将输入序列重复进行自注意力计算n次，每次使用不同的权重矩阵,得到n个注意力向量序列。然后将这n个序列拼接并线性转换，得到最终的序列表示,即:</p> <p></p><p><mjx-container jax="CHTML" display="true" class="MathJax"><mjx-math display="true" class="MJX-TEX"><mjx-mi class="mjx-i"><mjx-c c="M"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c c="u"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c c="l"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c c="t"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c c="i"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c c="H"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c c="e"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c c="a"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c c="d"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c c="("></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c c="Q"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c c=","></mjx-c></mjx-mo><mjx-mi space="2" class="mjx-i"><mjx-c c="K"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c c=","></mjx-c></mjx-mo><mjx-mi space="2" class="mjx-i"><mjx-c c="V"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c c=")"></mjx-c></mjx-mo><mjx-mo space="4" class="mjx-n"><mjx-c c="="></mjx-c></mjx-mo><mjx-mi space="4" class="mjx-i"><mjx-c c="c"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c c="o"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c c="n"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c c="c"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c c="a"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c c="t"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c c="("></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c c="h"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c c="e"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c c="a"></mjx-c></mjx-mi><mjx-msub><mjx-mi noIC="true" class="mjx-i"><mjx-c c="d"></mjx-c></mjx-mi><mjx-script style="vertical-align:-0.15em;"><mjx-mn size="s" class="mjx-n"><mjx-c c="1"></mjx-c></mjx-mn></mjx-script></mjx-msub><mjx-mo class="mjx-n"><mjx-c c=","></mjx-c></mjx-mo><mjx-mo space="2" class="mjx-n"><mjx-c c="22EF"></mjx-c></mjx-mo><mjx-mo space="2" class="mjx-n"><mjx-c c=","></mjx-c></mjx-mo><mjx-mi space="2" class="mjx-i"><mjx-c c="h"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c c="e"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c c="a"></mjx-c></mjx-mi><mjx-msub><mjx-mi noIC="true" class="mjx-i"><mjx-c c="d"></mjx-c></mjx-mi><mjx-script style="vertical-align:-0.15em;"><mjx-mi size="s" class="mjx-i"><mjx-c c="n"></mjx-c></mjx-mi></mjx-script></mjx-msub><mjx-mo class="mjx-n"><mjx-c c=")"></mjx-c></mjx-mo><mjx-msub><mjx-mi noIC="true" class="mjx-i"><mjx-c c="W"></mjx-c></mjx-mi><mjx-script style="vertical-align:-0.15em;"><mjx-mi size="s" class="mjx-i"><mjx-c c="o"></mjx-c></mjx-mi></mjx-script></mjx-msub></mjx-math></mjx-container></p><p></p> <p></p><p><mjx-container jax="CHTML" display="true" class="MathJax"><mjx-math display="true" class="MJX-TEX"><mjx-mi class="mjx-i"><mjx-c c="w"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c c="h"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c c="e"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c c="r"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c c="e"></mjx-c></mjx-mi><mjx-mtext class="mjx-n"><mjx-c c="A0"></mjx-c></mjx-mtext><mjx-mi class="mjx-i"><mjx-c c="h"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c c="e"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c c="a"></mjx-c></mjx-mi><mjx-msub><mjx-mi noIC="true" class="mjx-i"><mjx-c c="d"></mjx-c></mjx-mi><mjx-script style="vertical-align:-0.15em;"><mjx-mi size="s" class="mjx-i"><mjx-c c="i"></mjx-c></mjx-mi></mjx-script></mjx-msub><mjx-mo space="4" class="mjx-n"><mjx-c c="="></mjx-c></mjx-mo><mjx-mi space="4" class="mjx-i"><mjx-c c="A"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c c="t"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c c="t"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c c="e"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c c="n"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c c="t"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c c="i"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c c="o"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c c="n"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c c="("></mjx-c></mjx-mo><mjx-msubsup><mjx-mi noIC="true" class="mjx-i"><mjx-c c="W"></mjx-c></mjx-mi><mjx-script style="vertical-align:-0.294em;"><mjx-mi size="s" class="mjx-i" style="margin-left:0.247em;"><mjx-c c="Q"></mjx-c></mjx-mi><mjx-spacer style="margin-top:0.18em;"></mjx-spacer><mjx-mi size="s" class="mjx-i"><mjx-c c="i"></mjx-c></mjx-mi></mjx-script></mjx-msubsup><mjx-mi class="mjx-i"><mjx-c c="Q"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c c=","></mjx-c></mjx-mo><mjx-msubsup space="2"><mjx-mi noIC="true" class="mjx-i"><mjx-c c="W"></mjx-c></mjx-mi><mjx-script style="vertical-align:-0.247em;"><mjx-mi size="s" class="mjx-i" style="margin-left:0.247em;"><mjx-c c="K"></mjx-c></mjx-mi><mjx-spacer style="margin-top:0.193em;"></mjx-spacer><mjx-mi size="s" class="mjx-i"><mjx-c c="i"></mjx-c></mjx-mi></mjx-script></mjx-msubsup><mjx-mi class="mjx-i"><mjx-c c="K"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c c=","></mjx-c></mjx-mo><mjx-msubsup space="2"><mjx-mi noIC="true" class="mjx-i"><mjx-c c="W"></mjx-c></mjx-mi><mjx-script style="vertical-align:-0.25em;"><mjx-mi size="s" class="mjx-i" style="margin-left:0.247em;"><mjx-c c="V"></mjx-c></mjx-mi><mjx-spacer style="margin-top:0.18em;"></mjx-spacer><mjx-mi size="s" class="mjx-i"><mjx-c c="i"></mjx-c></mjx-mi></mjx-script></mjx-msubsup><mjx-mi class="mjx-i"><mjx-c c="V"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c c=")"></mjx-c></mjx-mo></mjx-math></mjx-container></p>
每次计算一个序列的自注意力被称为一个&quot;头&quot;，因此，&quot;多头注意力&quot;就是多次进行自注意力计算。每个&quot;头&quot;可能对应着不同的问题，例如第一个&quot;头&quot;可能关注&quot;发生了什么&quot;，第二个&quot;头&quot;可能关注&quot;何时发生&quot;，第三个&quot;头&quot;可能关注&quot;与谁有关&quot;等等。<p></p> <p>多头注意力的计算过程与自注意力基本一致，但是使用了不同的权重矩阵（，并且将所有的注意力向量（一般情况下是8个）进行拼接，再乘以一个权重矩阵，最后得到的结果就是多头注意力的输出。在实际计算中，由于不同&quot;头&quot;的计算互不影响，可以同时计算所有的&quot;头&quot;，即并行计算，以提高计算效率。</p> <p>总的来说，多头注意力机制可以为每个单词学习到更丰富、更好的表示，每个&quot;头&quot;都能从不同的角度去理解序列中的每个单词。</p> <h2 id="transformer-网络"><a href="#transformer-网络" class="header-anchor">#</a> Transformer 网络</h2> <p>在Transformer网络中,Encoder和Decoder均由<strong>多头注意力层和全连接前馈网络</strong>组成,网络的高层结构如下:</p> <ul><li>Encoder由N个编码器块(Encoder Block)串联组成,每个编码器块包含:
<ul><li>一个多头注意力(Multi-Head Attention)层</li> <li>一个前馈全连接神经网络(Feed Forward Neural Network)</li></ul></li> <li>Decoder也由N个解码器块(Decoder Block)串联组成,每个解码器块包含:
<ul><li>一个多头注意力层</li> <li>一个对Encoder输出的多头注意力层</li> <li>一个前馈全连接神经网络</li></ul></li></ul> <p><img src="https://leafw-blog-pic.oss-cn-hangzhou.aliyuncs.com/1693034970510.png" alt=""></p> <p>我们以一个法语翻译成英语的例子来讲解这个过程：</p> <ol><li>首先，输入<strong>句子的嵌入会被传递到编码器块</strong>，该块具有多头注意力机制。将嵌入和权重矩阵计算出的Q，K和V值输入到这个模块，然后生成一个可以传递到前馈神经网络的矩阵，用于确定句子中有趣的特性。在Transformer的论文中，这个编码块会被重复N次，一般N的值为6。</li> <li>然后，编码器的<strong>输出会被输入到解码器块</strong>。解码器的任务是输出英文翻译。解码器块的每一步都会输入已经生成的翻译的前几个单词。当我们刚开始时，唯一知道的是翻译会以一个开始句子的标记开始。这个标记被输入到多头注意力块，并用于计算这个多头注意力块的Q，K和V。<strong>这个块的输出会用于生成下一个多头注意力块的Q矩阵，而编码器的输出会用于生成K和V</strong>。</li> <li><strong>解码器块的输出被输入到前馈神经网络</strong>，该网络的任务是预测句子中的下一个单词。
除了主要的编码器和解码器块，Transformer Network还有一些额外的特性：</li></ol> <ul><li>位置编码：对输入进行位置编码，以便在翻译中考虑单词在句子中的位置。使用一组正弦和余弦方程来实现。</li> <li>残差连接：除了将位置编码添加到嵌入中，还通过残差连接将它们传递到网络中。这与之前在ResNet中看到的残差连接类似，其目的是在整个架构中传递位置信息。</li> <li>Adenome层：Adenome层类似于BatchNorm层，其目的是传递位置信息。</li> <li>遮掩多头注意力：这只在训练过程中重要，它模拟网络在预测时的行为，看看给定正确的前半部分翻译，神经网络是否能准确地预测序列中的下一个单词。</li></ul> <h2 id="总结"><a href="#总结" class="header-anchor">#</a> 总结</h2> <p>Transformer网络通过引入自注意力和多头注意力等机制,实现了序列建模的质的飞跃,在机器翻译、文本摘要、问答系统等任务上都取得了极大的成功。研究表明,其并行计算结构也使Transformer网络相比RNN等模型具有显著的计算效率优势，如今百家争鸣的大模型底层其实也离不开它的身影，理解它对于学习那些大语言模型是非常有帮助的。</p></div></section> <footer class="page-edit"><!----> <!----></footer> <div class="page-nav"><p class="inner"><span class="prev"><a href="/AI/courses/DeepLearning Specialization/05 Sequence Models/Week3 blog.html" class="prev">
          解密序列到序列模型：从机器翻译到语音识别
        </a></span> <span class="next"><a href="/AI/机器学习/关联规则挖掘与Apriori算法.html">
          关联规则挖掘与Apriori算法
        </a></span></p></div> <div class="comments-wrapper"><!----></div></main></div> <!----></div> <ul class="sub-sidebar sub-sidebar-wrapper" style="width:0;" data-v-b57cc07c data-v-7dd95ae2></ul></div></div></div><div class="global-ui"><div class="back-to-ceiling" style="right:1rem;bottom:6rem;width:2.5rem;height:2.5rem;border-radius:.25rem;line-height:2.5rem;display:none;" data-v-c6073ba8 data-v-c6073ba8><svg t="1574745035067" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="5404" class="icon" data-v-c6073ba8><path d="M526.60727968 10.90185116a27.675 27.675 0 0 0-29.21455937 0c-131.36607665 82.28402758-218.69155461 228.01873535-218.69155402 394.07834331a462.20625001 462.20625001 0 0 0 5.36959153 69.94390903c1.00431239 6.55289093-0.34802892 13.13561351-3.76865779 18.80351572-32.63518765 54.11355614-51.75690182 118.55860487-51.7569018 187.94566865a371.06718723 371.06718723 0 0 0 11.50484808 91.98906777c6.53300375 25.50556257 41.68394495 28.14064038 52.69160883 4.22606766 17.37162448-37.73630017 42.14135425-72.50938081 72.80769204-103.21549295 2.18761121 3.04276886 4.15646224 6.24463696 6.40373557 9.22774369a1871.4375 1871.4375 0 0 0 140.04691725 5.34970492 1866.36093723 1866.36093723 0 0 0 140.04691723-5.34970492c2.24727335-2.98310674 4.21612437-6.18497483 6.3937923-9.2178004 30.66633723 30.70611158 55.4360664 65.4791928 72.80769147 103.21549355 11.00766384 23.91457269 46.15860503 21.27949489 52.69160879-4.22606768a371.15156223 371.15156223 0 0 0 11.514792-91.99901164c0-69.36717486-19.13165746-133.82216804-51.75690182-187.92578088-3.42062944-5.66790279-4.76302748-12.26056868-3.76865837-18.80351632a462.20625001 462.20625001 0 0 0 5.36959269-69.943909c-0.00994388-166.08943902-87.32547796-311.81420293-218.6915546-394.09823051zM605.93803103 357.87693858a93.93749974 93.93749974 0 1 1-187.89594924 6.1e-7 93.93749974 93.93749974 0 0 1 187.89594924-6.1e-7z" p-id="5405" data-v-c6073ba8></path><path d="M429.50777625 765.63860547C429.50777625 803.39355007 466.44236686 1000.39046097 512.00932183 1000.39046097c45.56695499 0 82.4922232-197.00623328 82.5015456-234.7518555 0-37.75494459-36.9345906-68.35043303-82.4922232-68.34111062-45.57627738-0.00932239-82.52019037 30.59548842-82.51086798 68.34111062z" p-id="5406" data-v-c6073ba8></path></svg></div></div></div>
    <script src="/assets/js/app.bd8004a5.js" defer></script><script src="/assets/js/7.d7dc3e09.js" defer></script><script src="/assets/js/2.d701f7d6.js" defer></script><script src="/assets/js/1.f54fd289.js" defer></script><script src="/assets/js/46.7cf49e15.js" defer></script>
  </body>
</html>
