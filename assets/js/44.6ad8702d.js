(window.webpackJsonp=window.webpackJsonp||[]).push([[44],{452:function(t,s,a){"use strict";a.r(s);var m=a(2),c=Object(m.a)({},(function(){var t=this,s=t._self._c;return s("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[s("h2",{attrs:{id:"word-embeddings-介绍"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#word-embeddings-介绍"}},[t._v("#")]),t._v(" Word Embeddings 介绍")]),t._v(" "),s("h3",{attrs:{id:"词嵌入的概念"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#词嵌入的概念"}},[t._v("#")]),t._v(" 词嵌入的概念")]),t._v(" "),s("p",[t._v("词嵌入是一种将词汇映射到密集向量的技术，这些向量捕捉词汇的语义和语法信息。在早期，我们用词汇表（vocabulary）和一个One-hot Vector 来表示词汇。比如，如果“man”是词典里的第5391个单词，那么我们就用一个在第5391位为1，其他位置都为0的向量来表示“man”。")]),t._v(" "),s("p",[t._v('这种表示方式的缺点是，它把每个词看作一个孤立的实体，让学习算法难以在词之间进行泛化。比如说，模型学到了"I want a glass of orange juice"是一句可能的句子，那么它看到"I want a glass of apple ..."时，也能推测出下一个词可能是"juice"。但在one-hot向量表示下，"apple"和"orange"之间的关系对模型来说，和"apple"与"man"、"woman"、"king"、"queen"之间的关系没有区别。因为任何两个不同的one-hot向量的点积都是零，任何两个向量之间的距离也都是一样的。这就导致模型无法理解"apple"和"orange"比"king"和"orange"、"queen"和"orange"更相似。')]),t._v(" "),s("p",[t._v('然而词嵌入通过连续的数值向量表示词汇，能够捕捉和表达词与词之间的相似性和关联性，比如，"apple"和"orange"的词嵌入向量会有很多相似的特征，因此模型能更快地从"orange juice"推断出"apple juice"。它能有效地改善模型对文本数据的理解能力，这对于很多NLP任务（如文本分类、情感分析、机器翻译等）都是非常关键的。')]),t._v(" "),s("h3",{attrs:{id:"词嵌入的可视化"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#词嵌入的可视化"}},[t._v("#")]),t._v(" 词嵌入的可视化")]),t._v(" "),s("p",[t._v('通过使用t-SNE算法（由Laurens van der Maaten和Geoff Hinton提出），我们可以把300维的词嵌入向量映射到二维空间进行可视化。可视化结果表明，词嵌入算法能学习到相关概念的相似特征，例如，词汇"man"和"woman"，"king"和"queen"等在二维空间中会被归类在一起。\n'),s("img",{attrs:{src:"https://leafw-blog-pic.oss-cn-hangzhou.aliyuncs.com/1692198509885.png",alt:""}})]),t._v(" "),s("p",[t._v('我们把这些特征化表示在一个高维空间（如300维）中的位置称为“嵌入”，原因在于你可以想象一个高维空间，然后把每个词（比如"orange"）的三维特征向量嵌入到这个300维空间的某个点。')]),t._v(" "),s("h3",{attrs:{id:"词嵌入在nlp中的应用背景"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#词嵌入在nlp中的应用背景"}},[t._v("#")]),t._v(" 词嵌入在NLP中的应用背景")]),t._v(" "),s("p",[t._v("词嵌入技术已被广泛应用于各种NLP任务，比如命名实体识别（Named Entity Recognition）任务，举个例子：")]),t._v(" "),s("div",{staticClass:"language- extra-class"},[s("pre",{pre:!0,attrs:{class:"language-text"}},[s("code",[t._v("Sally Johnson is an orange farmer\nRobert Lin is an apple farmer\n")])])]),s("p",[t._v("我们需要“Sally Johnson ”是人名，如果我们将“orange farmer”中的“orange”替换为其他的水果，如“apple”，我们的算法应该能够推广到这种情况并正确地将“Robert Lin”识别为人名。甚至我们的模型可能推测出训练集未出现过的单词，比如durian farmer，词嵌入可以告诉我们durian也是一种水果。")]),t._v(" "),s("p",[t._v("此外我们还可以利用词嵌入来进行"),s("strong",[t._v("迁移学习")]),t._v("。步骤如下：")]),t._v(" "),s("ul",[s("li",[t._v("从大规模的文本语料库中学习词嵌入，或者下载预训练的词嵌入。")]),t._v(" "),s("li",[t._v("将词嵌入迁移到新的任务中，在这个新的任务中，我们可能只有一个相对较小的标记训练集。")]),t._v(" "),s("li",[t._v("使用这些词嵌入来表示我们的单词。例如，我们可以使用300维的词嵌入来代替10000维的one-hot向量。")]),t._v(" "),s("li",[t._v("在新的任务上训练我们的模型。在训练过程中，我们可以选择继续微调词嵌入。")])]),t._v(" "),s("p",[t._v("词嵌入在命名实体识别、文本摘要等任务都有很好的作用，但是对于语言模型和机器翻译等任务，如果我们有大量的数据，那么词嵌入的效果可能并不明显。这是因为迁移学习在我们有大量的任务A的数据和较小的任务B的数据时最有用。")]),t._v(" "),s("p",[t._v("词嵌入和face encoding有一些相似之处。在面部识别中，我们会训练一个网络来学习一个128维的表示。然后，我们可以比较这些编码来判断这两张图片是否是同一张脸。在词嵌入中，我们也会学习一个固定的词嵌入，例如，我们可能会有一个固定的词汇表，包含10000个单词，我们会为每一个单词学习一个词嵌入。")]),t._v(" "),s("p",[s("img",{attrs:{src:"https://leafw-blog-pic.oss-cn-hangzhou.aliyuncs.com/1692199128665.png",alt:""}})]),t._v(" "),s("h3",{attrs:{id:"word-embeddings-的特点"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#word-embeddings-的特点"}},[t._v("#")]),t._v(" Word embeddings 的特点")]),t._v(" "),s("p",[t._v("词嵌入可以帮助理解和建立类比关系，我们看下面这张表：\n"),s("img",{attrs:{src:"https://leafw-blog-pic.oss-cn-hangzhou.aliyuncs.com/1692199459348.png",alt:""}}),t._v("\n“男人（man）之于女人（woman）就像国王（king）之于什么？”如果你有一个正确的词嵌入模型，答案将是“女王（queen）”。这是因为在词嵌入的向量空间中，“男人”与“女人”的向量差距主要表示性别的差异，同样，“国王”与“女王”的向量差距也表示性别的差异。其实这就是词嵌入（word embeddings）在"),s("strong",[t._v("类比推理")]),t._v("（analogy reasoning）中的应用。")]),t._v(" "),s("p",[t._v("词嵌入要做的是找到一个词w，使得"),s("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[s("mjx-math",{staticClass:"MJX-TEX"},[s("mjx-msub",[s("mjx-mi",{staticClass:"mjx-i",attrs:{noIC:"true"}},[s("mjx-c",{attrs:{c:"e"}})],1),s("mjx-script",{staticStyle:{"vertical-align":"-0.15em"}},[s("mjx-TeXAtom",{attrs:{size:"s"}},[s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"m"}})],1),s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"a"}})],1),s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"n"}})],1)],1)],1)],1),s("mjx-mo",{staticClass:"mjx-n",attrs:{space:"3"}},[s("mjx-c",{attrs:{c:"2212"}})],1),s("mjx-msub",{attrs:{space:"3"}},[s("mjx-mi",{staticClass:"mjx-i",attrs:{noIC:"true"}},[s("mjx-c",{attrs:{c:"e"}})],1),s("mjx-script",{staticStyle:{"vertical-align":"-0.15em"}},[s("mjx-TeXAtom",{attrs:{size:"s"}},[s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"w"}})],1),s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"o"}})],1),s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"m"}})],1),s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"a"}})],1),s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"n"}})],1)],1)],1)],1),s("mjx-mo",{staticClass:"mjx-n",attrs:{space:"4"}},[s("mjx-c",{attrs:{c:"2248"}})],1),s("mjx-msub",{attrs:{space:"4"}},[s("mjx-mi",{staticClass:"mjx-i",attrs:{noIC:"true"}},[s("mjx-c",{attrs:{c:"e"}})],1),s("mjx-script",{staticStyle:{"vertical-align":"-0.15em"}},[s("mjx-TeXAtom",{attrs:{size:"s"}},[s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"k"}})],1),s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"i"}})],1),s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"n"}})],1),s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"g"}})],1)],1)],1)],1),s("mjx-mo",{staticClass:"mjx-n",attrs:{space:"3"}},[s("mjx-c",{attrs:{c:"2212"}})],1),s("mjx-msub",{attrs:{space:"3"}},[s("mjx-mi",{staticClass:"mjx-i",attrs:{noIC:"true"}},[s("mjx-c",{attrs:{c:"e"}})],1),s("mjx-script",{staticStyle:{"vertical-align":"-0.15em"}},[s("mjx-mi",{staticClass:"mjx-i",attrs:{size:"s"}},[s("mjx-c",{attrs:{c:"w"}})],1)],1)],1)],1)],1),t._v("，其中e_man、e_woman、e_king和e_w是词向量。这意味着“Man”和“Woman”之间的向量差距与“King”和词w之间的向量差距相似。这样的算法实际上是可以找到正确答案的，比如在这个例子中，它会找到“Queen”。")],1),t._v(" "),s("p",[t._v("最常用的相似度函数是余弦相似度（cosine similarity）。这个函数将两个向量u和v的内积除以它们的欧几里得长度。如果u和v非常相似，它们的内积会比较大，余弦相似度也就越接近1。余弦相似度实际上是向量u和v之间的角度的余弦值。如果角度为0，则余弦相似度为1；如果角度为90度，则余弦相似度为0；如果角度为180度，意味着它们完全朝反方向，那么余弦相似度为-1。\n"),s("img",{attrs:{src:"https://leafw-blog-pic.oss-cn-hangzhou.aliyuncs.com/1692201206584.png",alt:""}}),t._v("\nword embeddings的另一个重要结果是它能学习的"),s("strong",[t._v("类比关系的泛化性")]),t._v("。例如，它可以学习到“男人”之于“女人”就像“男孩”之于“女孩”，或者“渥太华”（加拿大的首都）之于“加拿大”就像“内罗毕”（肯尼亚的首都）之于“肯尼亚”，等等。这种模式可以通过在大型文本语料库上运行词嵌入学习算法自动学习。")]),t._v(" "),s("p",[t._v("我们可以使用像t-SNE这样的算法将300维的数据映射到2D空间进行可视化，但需要注意，因为t-SNE是一种非线性映射，所以在2D空间中可能不能很好地保持原始300维空间中的类比关系。")]),t._v(" "),s("h3",{attrs:{id:"embedding-matrix"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#embedding-matrix"}},[t._v("#")]),t._v(" Embedding Matrix")]),t._v(" "),s("p",[t._v('当我们实现一个算法来学习词嵌入时，其实最终学习的是一个嵌入矩阵。举个例子，假设我们的词汇表有10000个单词，包括"a", "aaron"... "<UNK>"等。\n'),s("img",{attrs:{src:"https://leafw-blog-pic.oss-cn-hangzhou.aliyuncs.com/1692424979785.png",alt:""}}),t._v('\n我们将要学习一个嵌入矩阵E，这是300*10000的矩阵，这个矩阵的每一列都是词汇表中的10000个不同单词的嵌入向量。假设"Orange"这个单词的one-hot向量是'),s("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[s("mjx-math",{staticClass:"MJX-TEX"},[s("mjx-msub",[s("mjx-mi",{staticClass:"mjx-i",attrs:{noIC:"true"}},[s("mjx-c",{attrs:{c:"O"}})],1),s("mjx-script",{staticStyle:{"vertical-align":"-0.15em"}},[s("mjx-TeXAtom",{attrs:{size:"s"}},[s("mjx-mn",{staticClass:"mjx-n"},[s("mjx-c",{attrs:{c:"6"}}),s("mjx-c",{attrs:{c:"2"}}),s("mjx-c",{attrs:{c:"5"}}),s("mjx-c",{attrs:{c:"7"}})],1)],1)],1)],1)],1)],1),t._v(" ,如果将E与"),s("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[s("mjx-math",{staticClass:"MJX-TEX"},[s("mjx-msub",[s("mjx-mi",{staticClass:"mjx-i",attrs:{noIC:"true"}},[s("mjx-c",{attrs:{c:"O"}})],1),s("mjx-script",{staticStyle:{"vertical-align":"-0.15em"}},[s("mjx-TeXAtom",{attrs:{size:"s"}},[s("mjx-mn",{staticClass:"mjx-n"},[s("mjx-c",{attrs:{c:"6"}}),s("mjx-c",{attrs:{c:"2"}}),s("mjx-c",{attrs:{c:"5"}}),s("mjx-c",{attrs:{c:"7"}})],1)],1)],1)],1)],1)],1),t._v(" 相乘，那么就会得到一个300维的向量"),s("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[s("mjx-math",{staticClass:"MJX-TEX"},[s("mjx-msub",[s("mjx-mi",{staticClass:"mjx-i",attrs:{noIC:"true"}},[s("mjx-c",{attrs:{c:"e"}})],1),s("mjx-script",{staticStyle:{"vertical-align":"-0.15em"}},[s("mjx-mn",{staticClass:"mjx-n",attrs:{size:"s"}},[s("mjx-c",{attrs:{c:"6"}})],1)],1)],1),s("mjx-mn",{staticClass:"mjx-n"},[s("mjx-c",{attrs:{c:"2"}}),s("mjx-c",{attrs:{c:"5"}}),s("mjx-c",{attrs:{c:"7"}})],1)],1)],1),t._v('，表示单词"Orange"的300*1维的嵌入向量。\n在学习词嵌入的过程中，这种通过使用嵌入矩阵和one-hot vector 提取特定词的嵌入的方式是非常重要的。')],1),t._v(" "),s("h2",{attrs:{id:"如何使用word-embeddings"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#如何使用word-embeddings"}},[t._v("#")]),t._v(" 如何使用Word Embeddings")]),t._v(" "),s("h3",{attrs:{id:"词嵌入算法的学习"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#词嵌入算法的学习"}},[t._v("#")]),t._v(" 词嵌入算法的学习")]),t._v(" "),s("p",[t._v('在深度学习应用于词嵌入的历史上，人们最初使用的是相对复杂的算法。但随着时间的推移，研究人员发现，即使使用更为简单的算法，特别是在大型数据集上，仍然能得到非常好的结果。\n依然举个例子来辅助说明，假设我们需要使用神经网络构建一个语言模型，这个模型可以实现输入一句话然后预测序列的下一个单词，如"I want a glass of orange (juice)."\n'),s("img",{attrs:{src:"https://leafw-blog-pic.oss-cn-hangzhou.aliyuncs.com/1692425542922.png",alt:""}}),t._v("\n构建语言模型的步骤可以这样来:")]),t._v(" "),s("ol",[s("li",[t._v("先取出句子中的单词，构建一个与单词对应的one-hot vector")]),t._v(" "),s("li",[t._v("使用参数矩阵E和每一个one-hot vector相乘。我们可以得到一组嵌入向量，每个向量都是300维的。")]),t._v(" "),s("li",[t._v("我们将得到的所有的嵌入向量放入伸进网络中，后续的网络会将这些向量输入到softmax分类器中，分类器会从词汇表中输出预测的最后的单词。")]),t._v(" "),s("li",[t._v('如果在训练集中看到了"juice"这个单词，那么训练softmax分类器时，我们的目标其实就是得到"juice"这个单词。')]),t._v(" "),s("li",[t._v("每个隐藏层都有自己的参数,我们称为"),s("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[s("mjx-math",{staticClass:"MJX-TEX"},[s("mjx-msub",[s("mjx-mi",{staticClass:"mjx-i",attrs:{noIC:"true"}},[s("mjx-c",{attrs:{c:"W"}})],1),s("mjx-script",{staticStyle:{"vertical-align":"-0.15em"}},[s("mjx-mn",{staticClass:"mjx-n",attrs:{size:"s"}},[s("mjx-c",{attrs:{c:"1"}})],1)],1)],1),s("mjx-mo",{staticClass:"mjx-n"},[s("mjx-c",{attrs:{c:","}})],1),s("mjx-msub",{attrs:{space:"2"}},[s("mjx-mi",{staticClass:"mjx-i",attrs:{noIC:"true"}},[s("mjx-c",{attrs:{c:"B"}})],1),s("mjx-script",{staticStyle:{"vertical-align":"-0.15em"}},[s("mjx-mn",{staticClass:"mjx-n",attrs:{size:"s"}},[s("mjx-c",{attrs:{c:"1"}})],1)],1)],1)],1)],1),t._v(" softmax也有自己的参数"),s("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[s("mjx-math",{staticClass:"MJX-TEX"},[s("mjx-msub",[s("mjx-mi",{staticClass:"mjx-i",attrs:{noIC:"true"}},[s("mjx-c",{attrs:{c:"W"}})],1),s("mjx-script",{staticStyle:{"vertical-align":"-0.15em"}},[s("mjx-mn",{staticClass:"mjx-n",attrs:{size:"s"}},[s("mjx-c",{attrs:{c:"2"}})],1)],1)],1),s("mjx-mo",{staticClass:"mjx-n"},[s("mjx-c",{attrs:{c:","}})],1),s("mjx-msub",{attrs:{space:"2"}},[s("mjx-mi",{staticClass:"mjx-i",attrs:{noIC:"true"}},[s("mjx-c",{attrs:{c:"B"}})],1),s("mjx-script",{staticStyle:{"vertical-align":"-0.15em"}},[s("mjx-mn",{staticClass:"mjx-n",attrs:{size:"s"}},[s("mjx-c",{attrs:{c:"2"}})],1)],1)],1)],1)],1),t._v(".如果使用300维的词嵌入，那么6个单词得到这一层的输入将是18000维的向量。")],1),t._v(" "),s("li",[t._v("但这个方法显然会由于句子长度导致这个向量很大，因此有一个更常见的做法就是使用一个固定的历史窗口，比如这个窗口是4，那就表示总是根据前四个词来预测下一个词，这个4是算法的超参数，这样我们可以处理任意长度的句子，因为窗口大小是固定的，注意我们不用给这四个单词的不同位置使用不同的参数矩阵E。")])]),t._v(" "),s("p",[t._v('此外再举一个复杂的句子，训练集有这样一句话： "I want a glass of orange juice to go along with my cereal."\n'),s("img",{attrs:{src:"https://leafw-blog-pic.oss-cn-hangzhou.aliyuncs.com/1692426676192.png",alt:""}}),t._v("\n我们的问题不像是前一个通过前面单词预测下一个单词，而是根据目标词的左右四个词类预测中间的词，这种方法也可以用来学习词嵌入。另外还有更简单的学习问题，比如通过“juice”反向预测“orange”，也可以得到很好的结果。\n后面我们还会讲解的skip-gram model, Negative sampling 都是有效的学习word embedding方法。")]),t._v(" "),s("h3",{attrs:{id:"word2vec"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#word2vec"}},[t._v("#")]),t._v(" Word2Vec")]),t._v(" "),s("p",[t._v("Word2Vec算法是一个简单且高效的学习 word embedding 的方法，其中的一个主要模型叫做 skip-gram 模型，主要是通过创建监督学习问题的上下文和目标对来实现的。这个模型并不是要完美地解决监督学习问题，而是通过这个问题学习出好的 embedding。")]),t._v(" "),s("p",[t._v("在skip-gram模型中，我们首先随机选择一个"),s("strong",[t._v("上下文词")]),t._v('（context word），比如"orange"，然后我们会在这个词的某个窗口范围内（比如前后5或10个词）随机选择一个'),s("strong",[t._v("目标词")]),t._v("（target word）。我们的监督学习问题就是，给定上下文词，预测在一定范围内的随机选择的词是什么。")]),t._v(" "),s("p",[t._v("假设我们有一个包含10000个词的词汇表。我们要学习的是如何从某个上下文词映射到某个目标词。为了表示输入（例如单词“orange”），我们可以使用一个one-hot vector，然后乘以嵌入矩阵E，得到输入上下文词的嵌入向量。我们将这个向量馈入到一个softmax单元，得到输出y_hat。然后我们就可以通过softmax的损失函数优化所有的这些参数，得到一组好的嵌入向量。")]),t._v(" "),s("p",[s("img",{attrs:{src:"https://leafw-blog-pic.oss-cn-hangzhou.aliyuncs.com/1692427351374.png",alt:""}})]),t._v(" "),s("p",[t._v("这个算法的主要问题是计算速度。对于softmax模型，我们需要在所有的词汇中进行求和，这在词汇表较大时会变得非常慢。为了解决这个问题，我们可以使用分层softmax分类器（hierarchical softmax classifier）或者负采样（negative sampling）方法。")]),t._v(" "),s("p",[t._v("此外，选择上下文词（context word）的方法也是一个值得关注的话题。如果我们随机选择，那么常见的词（如“the”、“of”、“a”、“and”、“to”等）会非常频繁地出现，这可能会使得你的训练对偶主要由这些词组成。为了解决这个问题，我们可以使用不同的启发式方法来平衡常见词和不常见词的出现。")]),t._v(" "),s("h3",{attrs:{id:"负采样-negative-sampling"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#负采样-negative-sampling"}},[t._v("#")]),t._v(" 负采样 (Negative Sampling)")]),t._v(" "),s("p",[t._v("负采样的出现是为了解决Softmax计算代价过大的问题。负采样的基本思想是通过构造一个新的有监督学习问题来学习词嵌入。")]),t._v(" "),s("p",[t._v("具体来说，我们可以将一个词对（例如，“orange”和“juice”）视为一个正例，并标记为1，同时从词典中随机选择一个词（例如，“king”）与“orange”组成一个负例，并标记为0。这样，我们就构造出了一个新的监督学习问题：给定一个词对，我们需要预测这是一个上下文-目标词对（标签为1）还是一个随机词对（标签为0）。")]),t._v(" "),s("p",[t._v("用数学语言描述就是，给定一对词"),s("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[s("mjx-math",{staticClass:"MJX-TEX"},[s("mjx-mo",{staticClass:"mjx-n"},[s("mjx-c",{attrs:{c:"("}})],1),s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"c"}})],1),s("mjx-mo",{staticClass:"mjx-n"},[s("mjx-c",{attrs:{c:","}})],1),s("mjx-mi",{staticClass:"mjx-i",attrs:{space:"2"}},[s("mjx-c",{attrs:{c:"t"}})],1),s("mjx-mo",{staticClass:"mjx-n"},[s("mjx-c",{attrs:{c:")"}})],1)],1)],1),t._v("（分别表示上下文词和目标词），以及标签"),s("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[s("mjx-math",{staticClass:"MJX-TEX"},[s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"y"}})],1),s("mjx-mo",{staticClass:"mjx-n",attrs:{space:"4"}},[s("mjx-c",{attrs:{c:"2208"}})],1),s("mjx-TeXAtom",{attrs:{space:"4"}},[s("mjx-mn",{staticClass:"mjx-n"},[s("mjx-c",{attrs:{c:"0"}})],1),s("mjx-mo",{staticClass:"mjx-n"},[s("mjx-c",{attrs:{c:","}})],1),s("mjx-mn",{staticClass:"mjx-n",attrs:{space:"2"}},[s("mjx-c",{attrs:{c:"1"}})],1)],1)],1)],1),t._v("，我们可以通过逻辑回归模型来预测"),s("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[s("mjx-math",{staticClass:"MJX-TEX"},[s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"y"}})],1),s("mjx-mo",{staticClass:"mjx-n",attrs:{space:"4"}},[s("mjx-c",{attrs:{c:"="}})],1),s("mjx-mn",{staticClass:"mjx-n",attrs:{space:"4"}},[s("mjx-c",{attrs:{c:"1"}})],1)],1)],1),t._v("的概率。模型的具体表达式为：\n")],1),s("p",[s("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML",display:"true"}},[s("mjx-math",{staticClass:"MJX-TEX",attrs:{display:"true"}},[s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"P"}})],1),s("mjx-mo",{staticClass:"mjx-n"},[s("mjx-c",{attrs:{c:"("}})],1),s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"y"}})],1),s("mjx-mo",{staticClass:"mjx-n",attrs:{space:"4"}},[s("mjx-c",{attrs:{c:"="}})],1),s("mjx-mn",{staticClass:"mjx-n",attrs:{space:"4"}},[s("mjx-c",{attrs:{c:"1"}})],1),s("mjx-mo",{staticClass:"mjx-n"},[s("mjx-c",{attrs:{c:"|"}})],1),s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"c"}})],1),s("mjx-mo",{staticClass:"mjx-n"},[s("mjx-c",{attrs:{c:","}})],1),s("mjx-mi",{staticClass:"mjx-i",attrs:{space:"2"}},[s("mjx-c",{attrs:{c:"t"}})],1),s("mjx-mo",{staticClass:"mjx-n"},[s("mjx-c",{attrs:{c:")"}})],1),s("mjx-mo",{staticClass:"mjx-n",attrs:{space:"4"}},[s("mjx-c",{attrs:{c:"="}})],1),s("mjx-mi",{staticClass:"mjx-i",attrs:{space:"4"}},[s("mjx-c",{attrs:{c:"3C3"}})],1),s("mjx-mo",{staticClass:"mjx-n"},[s("mjx-c",{attrs:{c:"("}})],1),s("mjx-msubsup",[s("mjx-mi",{staticClass:"mjx-i",attrs:{noIC:"true"}},[s("mjx-c",{attrs:{c:"3B8"}})],1),s("mjx-script",{staticStyle:{"vertical-align":"-0.247em"}},[s("mjx-mi",{staticClass:"mjx-i",attrs:{size:"s"}},[s("mjx-c",{attrs:{c:"T"}})],1),s("mjx-spacer",{staticStyle:{"margin-top":"0.217em"}}),s("mjx-mi",{staticClass:"mjx-i",attrs:{size:"s"}},[s("mjx-c",{attrs:{c:"t"}})],1)],1)],1),s("mjx-msub",[s("mjx-mi",{staticClass:"mjx-i",attrs:{noIC:"true"}},[s("mjx-c",{attrs:{c:"e"}})],1),s("mjx-script",{staticStyle:{"vertical-align":"-0.15em"}},[s("mjx-mi",{staticClass:"mjx-i",attrs:{size:"s"}},[s("mjx-c",{attrs:{c:"c"}})],1)],1)],1),s("mjx-mo",{staticClass:"mjx-n"},[s("mjx-c",{attrs:{c:")"}})],1)],1)],1)],1),t._v("\n其中，"),s("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[s("mjx-math",{staticClass:"MJX-TEX"},[s("mjx-msub",[s("mjx-mi",{staticClass:"mjx-i",attrs:{noIC:"true"}},[s("mjx-c",{attrs:{c:"3B8"}})],1),s("mjx-script",{staticStyle:{"vertical-align":"-0.15em"}},[s("mjx-mi",{staticClass:"mjx-i",attrs:{size:"s"}},[s("mjx-c",{attrs:{c:"t"}})],1)],1)],1)],1)],1),t._v("是目标词"),s("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[s("mjx-math",{staticClass:"MJX-TEX"},[s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"t"}})],1)],1)],1),t._v("的参数向量，"),s("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[s("mjx-math",{staticClass:"MJX-TEX"},[s("mjx-msub",[s("mjx-mi",{staticClass:"mjx-i",attrs:{noIC:"true"}},[s("mjx-c",{attrs:{c:"e"}})],1),s("mjx-script",{staticStyle:{"vertical-align":"-0.15em"}},[s("mjx-mi",{staticClass:"mjx-i",attrs:{size:"s"}},[s("mjx-c",{attrs:{c:"c"}})],1)],1)],1)],1)],1),t._v("是上下文词"),s("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[s("mjx-math",{staticClass:"MJX-TEX"},[s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"c"}})],1)],1)],1),t._v("的嵌入向量，"),s("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[s("mjx-math",{staticClass:"MJX-TEX"},[s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"3C3"}})],1)],1)],1),t._v("是sigmoid函数。"),s("p"),t._v(" "),s("p",[t._v("这样，我们就将一个10,000维的Softmax问题，转化为了10,000个二分类问题，每个问题都是一个逻辑回归。在每次迭代中，我们只需要更新一部分的分类器，具体来说，就是一个正例对应的分类器，和"),s("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[s("mjx-math",{staticClass:"MJX-TEX"},[s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"k"}})],1)],1)],1),t._v("个随机选择的负例对应的分类器。这样，就大大降低了计算的复杂度。")],1),t._v(" "),s("p",[t._v("瓷碗，如何选择负例也是一个关键的问题，如果我们直接按照语料库中词的频率来采样，那么常见词（如“the”、“of”、“and”等）的出现频率会过高。反之，如果我们均匀地随机选择词，那么选择到的词可能并不能很好地反映实际的语言分布。我们可以使用一种这种的方式，按照次品的四分之三次方进行采样：\n")]),s("p",[s("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML",display:"true"}},[s("mjx-math",{staticClass:"MJX-TEX",attrs:{display:"true"}},[s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"P"}})],1),s("mjx-mo",{staticClass:"mjx-n"},[s("mjx-c",{attrs:{c:"("}})],1),s("mjx-msub",[s("mjx-mi",{staticClass:"mjx-i",attrs:{noIC:"true"}},[s("mjx-c",{attrs:{c:"W"}})],1),s("mjx-script",{staticStyle:{"vertical-align":"-0.15em"}},[s("mjx-mi",{staticClass:"mjx-i",attrs:{size:"s"}},[s("mjx-c",{attrs:{c:"i"}})],1)],1)],1),s("mjx-mo",{staticClass:"mjx-n"},[s("mjx-c",{attrs:{c:")"}})],1),s("mjx-mo",{staticClass:"mjx-n",attrs:{space:"4"}},[s("mjx-c",{attrs:{c:"="}})],1),s("mjx-mfrac",{attrs:{space:"4"}},[s("mjx-frac",{attrs:{type:"d"}},[s("mjx-num",[s("mjx-nstrut",{attrs:{type:"d"}}),s("mjx-mrow",[s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"f"}})],1),s("mjx-mo",{staticClass:"mjx-n"},[s("mjx-c",{attrs:{c:"("}})],1),s("mjx-msub",[s("mjx-mi",{staticClass:"mjx-i",attrs:{noIC:"true"}},[s("mjx-c",{attrs:{c:"w"}})],1),s("mjx-script",{staticStyle:{"vertical-align":"-0.15em"}},[s("mjx-mi",{staticClass:"mjx-i",attrs:{size:"s"}},[s("mjx-c",{attrs:{c:"i"}})],1)],1)],1),s("mjx-msup",[s("mjx-mo",{staticClass:"mjx-n"},[s("mjx-c",{attrs:{c:")"}})],1),s("mjx-script",{staticStyle:{"vertical-align":"0.363em"}},[s("mjx-TeXAtom",{attrs:{size:"s"}},[s("mjx-mn",{staticClass:"mjx-n"},[s("mjx-c",{attrs:{c:"3"}})],1),s("mjx-TeXAtom",[s("mjx-mo",{staticClass:"mjx-n"},[s("mjx-c",{attrs:{c:"/"}})],1)],1),s("mjx-mn",{staticClass:"mjx-n"},[s("mjx-c",{attrs:{c:"4"}})],1)],1)],1)],1)],1)],1),s("mjx-dbox",[s("mjx-dtable",[s("mjx-line",{attrs:{type:"d"}}),s("mjx-row",[s("mjx-den",[s("mjx-dstrut",{attrs:{type:"d"}}),s("mjx-mrow",[s("mjx-munderover",{attrs:{limits:"false"}},[s("mjx-mo",{staticClass:"mjx-sop"},[s("mjx-c",{attrs:{c:"2211"}})],1),s("mjx-script",{staticStyle:{"vertical-align":"-0.285em"}},[s("mjx-TeXAtom",{attrs:{size:"s"}},[s("mjx-mn",{staticClass:"mjx-n"},[s("mjx-c",{attrs:{c:"1"}}),s("mjx-c",{attrs:{c:"0"}}),s("mjx-c",{attrs:{c:"0"}}),s("mjx-c",{attrs:{c:"0"}}),s("mjx-c",{attrs:{c:"0"}})],1)],1),s("mjx-spacer",{staticStyle:{"margin-top":"0.276em"}}),s("mjx-TeXAtom",{attrs:{size:"s"}},[s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"j"}})],1),s("mjx-mo",{staticClass:"mjx-n"},[s("mjx-c",{attrs:{c:"="}})],1),s("mjx-mn",{staticClass:"mjx-n"},[s("mjx-c",{attrs:{c:"1"}})],1)],1)],1)],1),s("mjx-mi",{staticClass:"mjx-i",attrs:{space:"2"}},[s("mjx-c",{attrs:{c:"f"}})],1),s("mjx-mo",{staticClass:"mjx-n"},[s("mjx-c",{attrs:{c:"("}})],1),s("mjx-msub",[s("mjx-mi",{staticClass:"mjx-i",attrs:{noIC:"true"}},[s("mjx-c",{attrs:{c:"w"}})],1),s("mjx-script",{staticStyle:{"vertical-align":"-0.15em"}},[s("mjx-mi",{staticClass:"mjx-i",attrs:{size:"s"}},[s("mjx-c",{attrs:{c:"j"}})],1)],1)],1),s("mjx-msup",[s("mjx-mo",{staticClass:"mjx-n"},[s("mjx-c",{attrs:{c:")"}})],1),s("mjx-script",{staticStyle:{"vertical-align":"0.363em"}},[s("mjx-TeXAtom",{attrs:{size:"s"}},[s("mjx-mn",{staticClass:"mjx-n"},[s("mjx-c",{attrs:{c:"3"}})],1),s("mjx-TeXAtom",[s("mjx-mo",{staticClass:"mjx-n"},[s("mjx-c",{attrs:{c:"/"}})],1)],1),s("mjx-mn",{staticClass:"mjx-n"},[s("mjx-c",{attrs:{c:"4"}})],1)],1)],1)],1)],1)],1)],1)],1)],1)],1)],1)],1)],1)],1),t._v("\n其中，"),s("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[s("mjx-math",{staticClass:"MJX-TEX"},[s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"f"}})],1),s("mjx-mo",{staticClass:"mjx-n"},[s("mjx-c",{attrs:{c:"("}})],1),s("mjx-msub",[s("mjx-mi",{staticClass:"mjx-i",attrs:{noIC:"true"}},[s("mjx-c",{attrs:{c:"w"}})],1),s("mjx-script",{staticStyle:{"vertical-align":"-0.15em"}},[s("mjx-mi",{staticClass:"mjx-i",attrs:{size:"s"}},[s("mjx-c",{attrs:{c:"i"}})],1)],1)],1),s("mjx-mo",{staticClass:"mjx-n"},[s("mjx-c",{attrs:{c:")"}})],1)],1)],1),t._v("表示词"),s("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[s("mjx-math",{staticClass:"MJX-TEX"},[s("mjx-msub",[s("mjx-mi",{staticClass:"mjx-i",attrs:{noIC:"true"}},[s("mjx-c",{attrs:{c:"w"}})],1),s("mjx-script",{staticStyle:{"vertical-align":"-0.15em"}},[s("mjx-mi",{staticClass:"mjx-i",attrs:{size:"s"}},[s("mjx-c",{attrs:{c:"i"}})],1)],1)],1)],1)],1),t._v("在语料库中的频率，"),s("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[s("mjx-math",{staticClass:"MJX-TEX"},[s("mjx-munderover",{attrs:{limits:"false"}},[s("mjx-mo",{staticClass:"mjx-sop"},[s("mjx-c",{attrs:{c:"2211"}})],1),s("mjx-script",{staticStyle:{"vertical-align":"-0.285em"}},[s("mjx-mi",{staticClass:"mjx-i",attrs:{size:"s"}},[s("mjx-c",{attrs:{c:"n"}})],1),s("mjx-spacer",{staticStyle:{"margin-top":"0.284em"}}),s("mjx-TeXAtom",{attrs:{size:"s"}},[s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"j"}})],1),s("mjx-mo",{staticClass:"mjx-n"},[s("mjx-c",{attrs:{c:"="}})],1),s("mjx-mn",{staticClass:"mjx-n"},[s("mjx-c",{attrs:{c:"0"}})],1)],1)],1)],1),s("mjx-mi",{staticClass:"mjx-i",attrs:{space:"2"}},[s("mjx-c",{attrs:{c:"f"}})],1),s("mjx-mo",{staticClass:"mjx-n"},[s("mjx-c",{attrs:{c:"("}})],1),s("mjx-msub",[s("mjx-mi",{staticClass:"mjx-i",attrs:{noIC:"true"}},[s("mjx-c",{attrs:{c:"w"}})],1),s("mjx-script",{staticStyle:{"vertical-align":"-0.15em"}},[s("mjx-mi",{staticClass:"mjx-i",attrs:{size:"s"}},[s("mjx-c",{attrs:{c:"j"}})],1)],1)],1),s("mjx-msup",[s("mjx-mo",{staticClass:"mjx-n"},[s("mjx-c",{attrs:{c:")"}})],1),s("mjx-script",{staticStyle:{"vertical-align":"0.363em"}},[s("mjx-TeXAtom",{attrs:{size:"s"}},[s("mjx-mn",{staticClass:"mjx-n"},[s("mjx-c",{attrs:{c:"3"}})],1),s("mjx-TeXAtom",[s("mjx-mo",{staticClass:"mjx-n"},[s("mjx-c",{attrs:{c:"/"}})],1)],1),s("mjx-mn",{staticClass:"mjx-n"},[s("mjx-c",{attrs:{c:"4"}})],1)],1)],1)],1)],1)],1),t._v("是一个归一化因子，保证所有词的采样概率之和为1。这种方式虽然缺乏理论支持，但在实践中表现良好，已经被许多研究者采用。"),s("p"),t._v(" "),s("p",[t._v("负采样是一种高效的词嵌入学习算法，通过将一个多分类问题转化为多个二分类问题，显著减小了计算复杂度。虽然我们可以自行训练词嵌入，但在许多情况下，直接使用开源的预训练词嵌入是一个更快的解决方案。")]),t._v(" "),s("h3",{attrs:{id:"glove-word-vectors"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#glove-word-vectors"}},[t._v("#")]),t._v(" Glove Word Vectors")]),t._v(" "),s("p",[t._v("接下再介绍一种在NLP社区中具有一些影响力的word embedding计算算法-GloVe算法。尽管GloVe算法没有像Word2Vec或skip-gram模型那样被广泛使用，但它依然拥有一些热衷的支持者，主要是因为它的简单性。")]),t._v(" "),s("p",[t._v("它的主要概念是根据全局词频统计信息来生成词向量，它定义了一个共现矩阵X，其中"),s("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[s("mjx-math",{staticClass:"MJX-TEX"},[s("mjx-msub",[s("mjx-mi",{staticClass:"mjx-i",attrs:{noIC:"true"}},[s("mjx-c",{attrs:{c:"X"}})],1),s("mjx-script",{staticStyle:{"vertical-align":"-0.15em"}},[s("mjx-TeXAtom",{attrs:{size:"s"}},[s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"i"}})],1),s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"j"}})],1)],1)],1)],1)],1)],1),t._v("表示词i在词j的上下文中出现的次数，表示了两个词一起出现的频率。\nGlove的目标是优化一个目标函数如下所示，使得词向量的内积能很好的预测两个词在一起出现的频率。对于出现频率为0的词对，定义log0=0。\n")],1),s("p",[s("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML",display:"true"}},[s("mjx-math",{staticClass:"MJX-TEX",attrs:{display:"true"}},[s("mjx-munderover",[s("mjx-over",{staticStyle:{"padding-bottom":"0.184em"}},[s("mjx-TeXAtom",{attrs:{size:"s"}},[s("mjx-mn",{staticClass:"mjx-n"},[s("mjx-c",{attrs:{c:"1"}}),s("mjx-c",{attrs:{c:"0"}}),s("mjx-c",{attrs:{c:"0"}}),s("mjx-c",{attrs:{c:"0"}}),s("mjx-c",{attrs:{c:"0"}})],1)],1)],1),s("mjx-box",[s("mjx-munder",[s("mjx-row",[s("mjx-base",{staticStyle:{"padding-left":"0.162em"}},[s("mjx-mo",{staticClass:"mjx-lop"},[s("mjx-c",{attrs:{c:"2211"}})],1)],1)],1),s("mjx-row",[s("mjx-under",{staticStyle:{"padding-top":"0.167em","padding-left":"0.31em"}},[s("mjx-TeXAtom",{attrs:{size:"s"}},[s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"i"}})],1),s("mjx-mo",{staticClass:"mjx-n"},[s("mjx-c",{attrs:{c:"="}})],1),s("mjx-mn",{staticClass:"mjx-n"},[s("mjx-c",{attrs:{c:"1"}})],1)],1)],1)],1)],1)],1)],1),s("mjx-munderover",{attrs:{space:"2"}},[s("mjx-over",{staticStyle:{"padding-bottom":"0.184em"}},[s("mjx-TeXAtom",{attrs:{size:"s"}},[s("mjx-mn",{staticClass:"mjx-n"},[s("mjx-c",{attrs:{c:"1"}}),s("mjx-c",{attrs:{c:"0"}}),s("mjx-c",{attrs:{c:"0"}}),s("mjx-c",{attrs:{c:"0"}}),s("mjx-c",{attrs:{c:"0"}})],1)],1)],1),s("mjx-box",[s("mjx-munder",[s("mjx-row",[s("mjx-base",{staticStyle:{"padding-left":"0.162em"}},[s("mjx-mo",{staticClass:"mjx-lop"},[s("mjx-c",{attrs:{c:"2211"}})],1)],1)],1),s("mjx-row",[s("mjx-under",{staticStyle:{"padding-top":"0.167em","padding-left":"0.286em"}},[s("mjx-TeXAtom",{attrs:{size:"s"}},[s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"j"}})],1),s("mjx-mo",{staticClass:"mjx-n"},[s("mjx-c",{attrs:{c:"="}})],1),s("mjx-mn",{staticClass:"mjx-n"},[s("mjx-c",{attrs:{c:"1"}})],1)],1)],1)],1)],1)],1)],1),s("mjx-mi",{staticClass:"mjx-i",attrs:{space:"2"}},[s("mjx-c",{attrs:{c:"f"}})],1),s("mjx-mo",{staticClass:"mjx-n"},[s("mjx-c",{attrs:{c:"("}})],1),s("mjx-msub",[s("mjx-mi",{staticClass:"mjx-i",attrs:{noIC:"true"}},[s("mjx-c",{attrs:{c:"X"}})],1),s("mjx-script",{staticStyle:{"vertical-align":"-0.15em"}},[s("mjx-TeXAtom",{attrs:{size:"s"}},[s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"i"}})],1),s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"j"}})],1)],1)],1)],1),s("mjx-mo",{staticClass:"mjx-n"},[s("mjx-c",{attrs:{c:")"}})],1),s("mjx-mo",{staticClass:"mjx-n"},[s("mjx-c",{attrs:{c:"("}})],1),s("mjx-msubsup",[s("mjx-mi",{staticClass:"mjx-i",attrs:{noIC:"true"}},[s("mjx-c",{attrs:{c:"3B8"}})],1),s("mjx-script",{staticStyle:{"vertical-align":"-0.247em"}},[s("mjx-mi",{staticClass:"mjx-i",attrs:{size:"s"}},[s("mjx-c",{attrs:{c:"T"}})],1),s("mjx-spacer",{staticStyle:{"margin-top":"0.193em"}}),s("mjx-mi",{staticClass:"mjx-i",attrs:{size:"s"}},[s("mjx-c",{attrs:{c:"i"}})],1)],1)],1),s("mjx-msub",[s("mjx-mi",{staticClass:"mjx-i",attrs:{noIC:"true"}},[s("mjx-c",{attrs:{c:"e"}})],1),s("mjx-script",{staticStyle:{"vertical-align":"-0.15em"}},[s("mjx-mi",{staticClass:"mjx-i",attrs:{size:"s"}},[s("mjx-c",{attrs:{c:"j"}})],1)],1)],1),s("mjx-mo",{staticClass:"mjx-n",attrs:{space:"3"}},[s("mjx-c",{attrs:{c:"+"}})],1),s("mjx-msub",{attrs:{space:"3"}},[s("mjx-mi",{staticClass:"mjx-i",attrs:{noIC:"true"}},[s("mjx-c",{attrs:{c:"b"}})],1),s("mjx-script",{staticStyle:{"vertical-align":"-0.15em"}},[s("mjx-mi",{staticClass:"mjx-i",attrs:{size:"s"}},[s("mjx-c",{attrs:{c:"i"}})],1)],1)],1),s("mjx-mo",{staticClass:"mjx-n",attrs:{space:"3"}},[s("mjx-c",{attrs:{c:"+"}})],1),s("mjx-msubsup",{attrs:{space:"3"}},[s("mjx-mi",{staticClass:"mjx-i",attrs:{noIC:"true"}},[s("mjx-c",{attrs:{c:"b"}})],1),s("mjx-script",{staticStyle:{"vertical-align":"-0.247em"}},[s("mjx-mo",{staticClass:"mjx-n",attrs:{size:"s"}},[s("mjx-c",{attrs:{c:"2032"}})],1),s("mjx-spacer",{staticStyle:{"margin-top":"0.223em"}}),s("mjx-mi",{staticClass:"mjx-i",attrs:{size:"s"}},[s("mjx-c",{attrs:{c:"j"}})],1)],1)],1),s("mjx-mo",{staticClass:"mjx-n",attrs:{space:"3"}},[s("mjx-c",{attrs:{c:"2212"}})],1),s("mjx-mi",{staticClass:"mjx-n",attrs:{space:"3"}},[s("mjx-c",{attrs:{c:"l"}}),s("mjx-c",{attrs:{c:"o"}}),s("mjx-c",{attrs:{c:"g"}})],1),s("mjx-mo",{staticClass:"mjx-n"},[s("mjx-c",{attrs:{c:"2061"}})],1),s("mjx-TeXAtom",{attrs:{space:"2"}},[s("mjx-msub",[s("mjx-mi",{staticClass:"mjx-i",attrs:{noIC:"true"}},[s("mjx-c",{attrs:{c:"X"}})],1),s("mjx-script",{staticStyle:{"vertical-align":"-0.15em"}},[s("mjx-TeXAtom",{attrs:{size:"s"}},[s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"i"}})],1),s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"j"}})],1)],1)],1)],1)],1),s("mjx-msup",[s("mjx-mo",{staticClass:"mjx-n"},[s("mjx-c",{attrs:{c:")"}})],1),s("mjx-script",{staticStyle:{"vertical-align":"0.413em"}},[s("mjx-mn",{staticClass:"mjx-n",attrs:{size:"s"}},[s("mjx-c",{attrs:{c:"2"}})],1)],1)],1)],1)],1)],1),t._v("\n对于词频非常高或者非常低的词，GloVe使用了一个权重函数f来平衡他们的贡献，避免让常见词过度占据权重，同时保证不常见词也有一定的权重。在训练过程中，对于每个词，GloVe会生成两组词向量θ和e，然后取平均值作为最终的词向量。这是因为在GloVe中，θ和e扮演了对称的角色。"),s("p"),t._v(" "),s("p",[t._v("GloVe的一个优点是，它能够明确地利用全局统计信息，一次考虑所有的共现统计数据。相比之下，Word2Vec模型一次只考虑一个上下文窗口。但在处理词义消歧和OOV（词汇表外的词）问题上，GloVe还存在一些局限性。例如，它不能为一个词的不同含义生成不同的词向量；对于未知词，只能分配一个共享的嵌入向量。")]),t._v(" "),s("p",[t._v("尽管GloVe存在一些问题，但它为自然语言处理提供了一种有效的词嵌入计算方法，并激发了许多研究工作来进一步改进这种方法。")]),t._v(" "),s("h2",{attrs:{id:"word-embeddings-的应用"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#word-embeddings-的应用"}},[t._v("#")]),t._v(" Word Embeddings 的应用")]),t._v(" "),s("h3",{attrs:{id:"情感分类-sentiment-classification"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#情感分类-sentiment-classification"}},[t._v("#")]),t._v(" 情感分类 (Sentiment classification)")]),t._v(" "),s("p",[t._v("情感分类是NLP中最重要的基础模块之一，其任务是根据文本内容确定其情感倾向，如喜欢或不喜欢某事物。尽管数据标注的挑战存在，但通过词嵌入技术，我们仍然可以使用规模适中的标签训练集来构建有效的情感分类器。")]),t._v(" "),s("p",[t._v("情感分类的一个应用例子是预测餐厅的星级评分。通过训练一个系统，将评论（X）映射为星级评分（Y），可以用于监控人们对你的餐厅的评论，从而了解餐厅的表现情况。")]),t._v(" "),s("p",[t._v("下面是使用词嵌入构建情感分类器的两种算法：")]),t._v(" "),s("ol",[s("li",[s("strong",[t._v("简单的情感分类模型：")]),t._v(" 对句子中的每个词在字典中进行查找，然后找到其一位有效编码，并通过嵌入矩阵将其转换为嵌入向量。这个嵌入矩阵可能是从大规模的文本语料库（例如100亿词）中学习得来的。然后，将这些嵌入向量进行求和或平均，以得到一个300维的特征向量，将其传入softmax分类器，从而得出预测结果。这种方法适用于各种长度的评论，但其问题是它忽略了词序，可能会误判含有大量正面词汇的负面评论。")]),t._v(" "),s("li",[s("strong",[t._v("使用RNN的情感分类模型：")]),t._v(" 除了将句子中的每个词转化为嵌入向量外，我们还可以将这些嵌入向量作为RNN的输入。RNN将在最后一个时间步长计算出表示，用于预测结果。这种模型可以更好地考虑到词序，避免了简单模型中的问题。")])]),t._v(" "),s("p",[t._v("这两种模型都可以利用词嵌入对大规模未标记数据中的信息进行编码，并将这些信息应用到小规模的标签训练集中，从而提高情感分类的效果。")]),t._v(" "),s("h3",{attrs:{id:"消除偏见"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#消除偏见"}},[t._v("#")]),t._v(" 消除偏见")]),t._v(" "),s("p",[t._v('机器学习和AI算法正在越来越多地被信任，用来帮助或做出极其重要的决策。所以我们要尽可能地确保它们不带有不良的偏见，比如性别偏见、种族偏见等。word embeddings 可以学习出类比，如 "man is to woman as king is to queen"（男人之于女人如同国王之于王后），但是有时候它也可能输出不良的类比，比如 "man is to computer programmer as woman is to homemaker"（男人之于程序员如同女人之于家庭主妇）。这样的结果显然是不公正的，而且强化了不健康的性别刻板印象。')]),t._v(" "),s("p",[t._v("词嵌入可以反映出训练模型所用文本的性别、种族、年龄、性取向等偏见。因为机器学习算法正在被用来做出很重要的决定，例如大学录取、工作找寻、贷款申请、甚至是刑事司法系统的量刑指南等，所以尽可能减少或消除这些不良偏见非常重要。")]),t._v(" "),s("p",[t._v("词嵌入偏见减少的步骤大概如下所示：")]),t._v(" "),s("ol",[s("li",[t._v('首先，识别出我们想要减少或消除的特定偏见的方向。比如，在处理性别偏见时，我们可以通过计算"he"（他）和"she"（她）等词的嵌入向量差值的平均值，来找出性别偏见的方向。')]),t._v(" "),s("li",[t._v('进行"中和"步骤，对于非定义性的词，将其投影以消除偏见。比如，"doctor"（医生）和"babysitter"（保姆）这样的词我们希望是性别中性的，所以我们将其投影到无关偏见的方向。')]),t._v(" "),s("li",[t._v('进行"均衡"步骤，对于一些词对（如grandmother-grandfather, girl-boy），我们希望他们的嵌入只在性别方面有差异。最后的"均衡"步骤就是确保像"grandmother"（祖母）和"grandfather"（祖父）这样的词与应该是性别中性的词（如"babysitter"（保姆）或"doctor"（医生））的相似性或距离完全一样。')])]),t._v(" "),s("p",[t._v("综上，本文的主要围绕Word Embeddings展开了一系列的知识点的介绍，包括它的概念，常见算法，使用场景等。这些知识点很多都是出自于一些经典的论文中，希望有空可以仔细阅读一下原文，才能更好的理解和掌握它们。")]),t._v(" "),s("h2",{attrs:{id:"附录-相关文献"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#附录-相关文献"}},[t._v("#")]),t._v(" 附录-相关文献")]),t._v(" "),s("ol",[s("li",[t._v("Van der Maaten L, Hinton G. Visualizing data using t-SNE[J]. Journal of machine learning research, 2008, 9(11).")]),t._v(" "),s("li",[t._v("Taigman Y, Yang M, Ranzato M A, et al. Deepface: Closing the gap to human-level performance in face verification[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2014: 1701-1708.")]),t._v(" "),s("li",[t._v("Taigman Y, Yang M, Ranzato M A, et al. Deepface: Closing the gap to human-level performance in face verification[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2014: 1701-1708.")]),t._v(" "),s("li",[t._v("Bengio Y, Ducharme R, Vincent P. A neural probabilistic language model[J]. Advances in neural information processing systems, 2000, 13.")]),t._v(" "),s("li",[t._v("Mikolov T, Chen K, Corrado G, et al. Efficient estimation of word representations in vector space[J]. arXiv preprint arXiv:1301.3781, 2013.")]),t._v(" "),s("li",[t._v("Mikolov T, Sutskever I, Chen K, et al. Distributed representations of words and phrases and their compositionality[J]. Advances in neural information processing systems, 2013, 26.")]),t._v(" "),s("li",[t._v("Pennington J, Socher R, Manning C D. Glove: Global vectors for word representation[C]//Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP). 2014: 1532-1543.")]),t._v(" "),s("li",[t._v("Bolukbasi T, Chang K W, Zou J Y, et al. Man is to computer programmer as woman is to homemaker? debiasing word embeddings[J]. Advances in neural information processing systems, 2016, 29.")])])],1)}),[],!1,null,null,null);s.default=c.exports}}]);